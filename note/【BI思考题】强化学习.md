# 【BI思考题】强化学习

**1.机器学习中监督学习，非监督学习，强化学习有何区别**

机器学习分为监督学习，非监督学习和强化学习三大类。监督学习是我们比较常见的分类回归问题，他们的特点在于，对于每个样本，都标记有一个已知的真实label，而监督学习的任务是根据已知样本的label在训练过程中去拟合对应的真实label，并在测试过程中对未知样本进行预测。

非监督学习和监督学习的区别在于样本并没有已知的label，训练过程中是挖掘样本之间的区别性和相关性，并根据发现的规律去分离和聚合样本，代表性任务是聚类。

强化学习跟监督学习的差别在于，其训练过程输出的并没有对应的可参考的真实值（label），而是一个事后给出的奖励值。强化学习看重的是行为序列下的长期收益，监督学习则关注的是预测值与label的误差。与非监督学习的差别在于，强化学习是有输出值的。另外强化学习和另两个分支的共同区别是，强化学习的样本是有前后依赖关系的，而监督学习和非监督学习的样本都是相互独立的。



**2.什么是策略网络，价值网络，有何区别**

- 策略网络是一个神经网络模型，通过观察环境状态s直接预测目前最该执行的策略a，他的组成形式是一连串的状态行动配对列表

$$
S=\left \{ (s_0,a_0),(s_1,a_1),...,(s_n,a_n) \right \}
$$

其学习的目标是对当前环境所采取的策略，而不是最后的期望值。其最优策略函数是
$$
\pi=\arg \max_\pi [\sum_{t \geq 0}\gamma ^{t} r_t| \pi]
$$


- 估值网络是要通过计算在当前状态s的累计分数的期望，通过整个数值网络给状态赋予一个分数。整个训练过程是对于价值的迭代，通过价值函数的更新来更新策略，

$$
V^{\pi}(S)=E\left [ \sum_{t\geq 0}^{}\gamma ^{t} r_t |s_0=s,\pi\right ]
$$

通过策略产生新的状态和即时奖励，进而更新价值参数，一直更新下去直到直到损失函数收敛。



**3.请简述MCTS原理，四个步骤，select，expansion，simulation，backpropagation**

MCTS算法的本质是一棵搜索树模型，从根结点出发沿着树到一个未完全展开的节点路径。再从这个节点中选取一个未被访问过的子节点来进行一次模拟，模拟的结果反向传播到这条路径上的父节点，一直到根节点，并更新节点的统计信息。在搜索结束后，可以根据收集的统计信息来决定下一步怎么走。

MCTS算法可以分为下面四个步骤：

1. 选择（select）：从根节点开始，找到其子节点（在游戏中就是在当前状态下可以执行的action到达下一个状态），并选择一个最有价值的子节点，而判断子节点的价值要计算UCT
   $$
   UCT(v_i,v)=\frac{Q(v_i)}{N(v_i)}+c \sqrt{\frac{\log N(v))}{N(v_i))}}
   $$
   其中$\frac{Q(v_i)}{N(v_i)}$代表子节点$v_i$的胜率估计。但是为了让更多的未被访问点也有机会在下一次模拟时被选中，则加入了第二个部分，其中$N$表示节点的访问次数，c用于调节探索与利用的超参数。

2. 扩展（expansion）：当搜索到未完全展开的叶子节点后，对叶子节点进行扩展成多个子节点。

3. 模拟（simulation）：用rollout策略（蒙特卡洛思想）快速模拟多次实验，得到最终的模拟结果胜，败或者平局。模拟到最终状态时可以得到模拟分数。

4. 反向传播（backpropagation）：根据模拟得分，来更新当前子节点的统计值，并回传到和父节点一直到根节点进行更新。



**4.假设你是抖音的技术负责人，强化学习在信息流推荐中会有怎样的作用，如果要进行强化学习，都要考虑哪些因素**

因为传统的推荐算法里，FM，协同过滤，wide&deep都只关注于当前信息而对用户做出推荐，但是没办法测量当前的推荐能对用户未来的行为造成的影响。所以可以用强化学习将推荐系统与用户建模成一个序列决策问题，不到那可以支持线上学习，有能更好的考虑长期的推荐收益。

我个人认为如果要对推荐系统使用强化学习的话，从强化学习要考虑的因素上来考虑，智能体应该是抖音目前使用的推荐算法，环境应该是用户的浏览日志，相对应的浏览时长（或比例：浏览时长/视频总时长），喜欢&收藏历史，动作为推荐算法推荐的前topk短视频，状态为新的喜欢&收藏以及浏览情况，奖励为是否浏览时长提高，或者喜欢&收藏比例提高，或者指定一个排序分数，策略应该是根据用户浏览日志调整topk的内容或者顺序。



**5.在自动驾驶中，如何使用强化学习进行训练，请说明简要的思路**

自动驾驶可以类比赛车游戏场景，智能体是车辆，环境是车辆周围的人，其他车，道路，光线等，动作为刹车，左转，右转，减速，加速。状态为道路情况以及车辆的情况，奖励是是否有危险行为或事故，乘客乘坐舒适度是否提升。策略为根据道路情况提速，变道，跟车，减速，紧急制动等等。

如果按照value-based来训练，则自动驾驶需要确定的是其价值函数Q，我觉得价值函数应该是安全无事故行驶时间，而目标是尽量的去最大化每一次开始行驶时的安全驾驶时间。

如果是按照policy-based来训练，则自动驾驶需要对状态进行策略制定，比如城市塞车，城市畅行，高速畅行，高速塞车等等，其对应的动作空间或者权重应有所不同。最终目的还是要少发生事故，提高安全行驶时间。



