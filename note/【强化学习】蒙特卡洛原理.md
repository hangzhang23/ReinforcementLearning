# 【强化学习】蒙特卡洛方法

## 蒙特卡洛原理

蒙特卡罗（MCM）方法一个用于估计价值函数和发现最优策略的学习方法。基于思想是：用事件发生的“频率”来替代事件发生的“概率”。进一步地说，我们可能无法准确知道一个事件发生的概率，这时候我们可以使用多次采样，使用该事件发生的频率来替代其发生的概率。

MCM具有如下的特点：

- 可以通过随机采样得到近似结果。

- 采样次数越多，估计结果越近似真实值。

和动态规划不同的地方在于，蒙特卡洛以随机样本估计期望，所以在计算状态价值函数和策略价值函数时，使用的是经验平均思想。在每一个**状态-动作**对上的回报都进行采样和平均，从而最终达到一个平稳的期望。

## 蒙特卡洛树搜索（MCTS）

### 1. 简单蒙特卡洛搜索

简单蒙特卡罗搜索基于一个强化学习模型$M_v$和一个模拟策略$π$.在此基础上，对于当前我们要选择动作的状态$S_t$, 对每一个可能采样的动作$a∈A$,都进行$K$轮采样，这样每个动作$a$都会得到K组经历完整的状态序列(episode)。即：
$$
\left\{S_{t},a,R^{k}_{t+1},S^{k}_{t+1},A^{k}_{t+1},......S^{k}_{T}\right\}^K_{k=1}∼Mv,π
$$
现在对于每个$(St,a)$组合，我们可以基于蒙特卡罗法来计算其动作价值函数并选择最优的动作了。
$$
Q(S_t,a)=\frac{1}{K} \sum^{K}_{k=1}G_t
$$

$$
a_t=\arg \max_{a \in A}Q(S_t,a)
$$

简单蒙特卡罗搜索和起前向搜索比起来，对于状态动作数量的处理能力上了一个数量级,可以处理中等规模的问题。但是假如我们的状态动作数量达到非常大的量级，比如围棋的级别，就需要使用蒙特卡洛树搜索了

### 2. 蒙特卡洛树搜索

#### 2.1 MCTS原理

MCTS摒弃了简单蒙特卡罗搜索里面对当前状态$S_t$每个动作都要进行K次模拟采样的做法，而是总共对当前状态$S_t$进行K次采样，这样采样到的动作只是动作全集$A$中的一部分。这样做大大降低了采样的数量和采样后的搜索计算。当然，代价是可能动作全集中的很多动作都没有采样到，可能错失好的动作选择，这是一个算法设计上的折衷。

在MCTS中，基于一个强化学习模型$M_v$和一个模拟策略$π$，当前状态$S_t$对应的完整的状态序列(episode)是这样的：
$$
\left\{S_{t},A^{k}_{t},R^{k}_{t+1},S^{k}_{t+1},A^{k}_{t+1},......S^{k}_{T}\right\}^K_{k=1}∼Mv,π
$$
采样完毕后，我们可以基于采样的结果构建一颗MCTS的搜索树，然后近似计算$Q(s_t,a)$和最大$Q(s_t,a)$对应的动作。
$$
Q(S_t,a)=\frac{1}{N(S_t,a)}\sum^{K}_{k=1}\sum^{T}_{u=t}1(S_{uk}=S_t,A_{uk}=a)G_u
$$

$$
a_t=\arg\max_{a\in A} Q(S_t,a)
$$

MCTS搜索的策略分为两个阶段：

第一个是树内策略(tree policy)：为当模拟采样得到的状态存在于当前的MCTS时使用的策略。树内策略可以使$ϵ−greedy$策略，随着模拟的进行策略可以得到持续改善，还可以使用上限置信区间算法UCT，这在棋类游戏中很普遍；

第二个是默认策略(default policy)：如果当前状态不在MCTS内，使用默认策略来完成整个状态序列的采样，并把当前状态纳入到搜索树中。默认策略可以使随机策略或基于目标价值函数的策略。

在棋类之类的零和问题和MCTS中延时奖励不同的是，中间状态是没有明确奖励的，我们只有在棋下完后知道输赢了才能对前面的动作进行状态奖励，对于这类问题我们的MCTS需要做一些结构上的细化。

### 2.2 UCT计算

棋类游戏中使用MCTS搜索时，更多用Upper Confidence Bound的方法代替$ϵ−greedy$。原因是贪婪策略在棋盘某一步状态下直接按照模拟选择高胜率动作，这样的话虽然在当前步是最优策略，但是在整个棋局对弈中，可能因为另一候选步对战棋局少数据不够而导致没有选择，实际却有可能是更好的一步。所以用UCT来代替贪婪策略。
$$
score=\frac{w_i}{n_i}c\sqrt{\frac{\ln N_i}{n_i}}
$$
UCT首先计算每一个可选动作节点对应的分数，这个分数考虑了历史最优策略和探索度。其中，$w_i$ 是 i 节点的胜利次数，$n_i$ 是i节点的模拟次数，$N_i$是所有模拟次数，c 是探索常数，理论值为$2–√2$，可根据经验调整，c 越大就越偏向于广度搜索，c 越小就越偏向于深度搜索。最后我们选择分数最高的动作节点。

### 2.3 棋类游戏MCTS搜索

对于MCTS的树结构，如果是最简单的方法，只需要在节点上保存状态对应的历史胜负记录。在每条边上保存采样的动作。这样MCTS的搜索需要走4步。

![](https://gitee.com/zhanghang23/picture_bed/raw/master/reinforcemence%20_learning/mcts.png)

第一步是选择(Selection):这一步会从根节点开始，每次都选一个“最值得搜索的子节点”，一般使用UCT选择分数最高的节点，直到来到一个“存在未扩展的子节点”的节点，如图中的 3/3 节点。之所以叫做“存在未扩展的子节点”，是因为这个局面存在未走过的后续着法，也就是MCTS中没有后续的动作可以参考了。这时我们进入第二步。

第二步是扩展(Expansion)，在这个搜索到的存在未扩展的子节点，加上一个0/0的子节点，表示没有历史记录参考。这时我们进入第三步。

第三步是仿真(simulation)，从上面这个没有试过的着法开始，用一个简单策略比如快速走子策略（Rollout policy）走到底，得到一个胜负结果。快速走子策略一般适合选择走子很快可能不是很精确的策略。因为如果这个策略走得慢，结果虽然会更准确，但由于耗时多了，在单位时间内的模拟次数就少了，所以不一定会棋力更强，有可能会更弱。这也是为什么我们一般只模拟一次，因为如果模拟多次，虽然更准确，但更慢。

第四步是回溯(backpropagation), 将我们最后得到的胜负结果回溯加到MCTS树结构上。注意除了之前的MCTS树要回溯外，新加入的节点也要加上一次胜负历史记录，如上图最右边所示。

以上就是MCTS搜索的整个过程。这4步一般是通用的，但是MCTS树结构上保存的内容而一般根据要解决的问题和建模的复杂度而不同。

### 2.4 AlphaZero中的MCTS

AlphaZero是AlphaGo的一个简化版本，但是同样用了价值策略网络和MCTS。MCTS在这个模型中主要是用在了两个地方：

- 首先是在价值策略网络中对神经网络部分得到的策略和价值进行当前棋局的扩展。

  ![](https://gitee.com/zhanghang23/picture_bed/raw/master/reinforcemence%20_learning/alphazero.png)

- 在前期模型较弱时作为对手对战。

## 3. 强化学习和深度学习的区别

深度学习本身属于一种静态学习方式，使用固定的数据集，通过加深神经网络层数来进一步提取特征。其目标函数本身要使得产生的结果要尽量去拟合数据集中的原始数据，最小化生成数据和原始数据得差异。其主要解决的还是感知问题。

强化学习是一种动态学习方式，它本身不需要提供数据，并且没有标签，智能体需要学习环境到行为之间的映射关系（决策），目标函数是最大化激励值，通过激励值的大小学习经验与决策反馈回智能体。其主要解决的是决策问题。